\documentclass[a4paper]{article}
\usepackage{multicol}
\usepackage{geometry}
\geometry{legalpaper, portrait, margin=1in}

\title{Machine Learning in Advertising Technology}
\author{Vangie Shue}

\begin{document}
\SweaveOpts{concordance=TRUE}

\maketitle
\section*{Abstract}
This project will explore the application of Machine Learning in Technology. Specifically, we will seek to develop a reliable process for developing a classification algorithm on given data.

\begin{multicols}{2}

\section*{Introduction}
Background on Advertising Technology use cases.
Background on Cafemom and the specific problem: classify hispanic users

\subsection*{Classifying Rare Events}
(1, pg141) Rare events are more statistically informative that zeros is seen in the variance matrix
(1, p142) When sampling, we must be careful not to select on X differently for the two samples.
(2) The problem is that maximum likelihood estimation of the logistic method is well-known to suffer from small-sample bias. The penalized likelihood or Firth method are the general approach to reducing small-sample bias.

\subsection*{Classification Algorithms}
\subsubsection*{Logistic Regression}
(4) glmnet

\section*{Methods}
Demdex has Traits\/Segments, uuid and traits\/segments collected
Table generating en\/es language to determine hispanic\/non-hispanic, 1 for es, 0 for en
\begin{itemize}
  \item segment\_hispanic: 80324851 total uuids
  \item segment\_hispanic: 1698878 hispanic (2 percent)
  \item segment\_hispanic2: 9733751 total uuids
  \item segment\_hispanic2: 1868091 hispanic (19 percent)
\end{itemize}

\subsection*{Logistic Regression}
We used \texttt{glmnet} to generate logistic regression model.
Then \texttt{predict.glm} is used to predict on the validation sample.
(3) We then determine the how low of a predicted probability is needed to accurately classify the hispanic division.

\subsection*{Random Forest or Ferns}

\subsection*{Support Vector Machines}

<<>>=
data(example)
@
The above is a snippet of code used. 

\section*{Results}
Compare time for execution \& accuracy between models\/methods. ex.Unweighted\/Non-filtered\/Logistic Regression

The below is a sample graph of data.
\begin{center}
Something like a plot centered.
\end{center}

\section*{Conclusion}
We demonstrated the application of Machine Learning in Advertising Technology, in particular for rare events.

Cannot accept comparisons without consideration to the implementation. Some may provide more "tuning" than other algorithms and therefore appear more accurate. However, we demonstrate which algorithm will work best for our usage.

\section*{Acknowledgements}
Patrick McCann \& Cafemom, Professor Mohri

\end{multicols}


\section*{References}
We used \texttt{RStudio Sweave} to build this \LaTeX{} document
\begin{itemize}
  \item 1 http://gking.harvard.edu/files/gking/files/0s.pdf (rare events)
  \item 2 http://www.statisticalhorizons.com/logistic-regression-for-rare-events (rare events)
  \item 4 http://cran.r-project.org/web/packages/glmnet/glmnet.pdf
  \item 3 http://stats.stackexchange.com/questions/25389/obtaining-predicted-values-y-1-or-0-from-a-logistic-regression-model-fit
  \item 5 http://machinelearningmastery.com/an-introduction-to-feature-selection/ (feature selection tips)
  \item best algorithms: http://www.researchgate.net/post/What\_is\_the\_best\_algorithm\_for\_classification\_task
  \item http://en.wikibooks.org/wiki/Data\_Mining\_Algorithms\_In\_R/Classification/SVM (SVM)
  \item http://www.jstatsoft.org/v61/i10/paper (random ferns)
  \item http://www.statmethods.net/advstats/cart.html (rpart and random forest)
\end{itemize}



\end{document}