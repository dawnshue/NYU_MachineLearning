\documentclass[a4paper]{article}
\usepackage{multicol}
\usepackage{geometry}
\geometry{legalpaper, portrait, margin=1in}

\title{Applying Machine Learning in Advertising Technology}
\author{Vangie Shue}

\begin{document}
\SweaveOpts{concordance=TRUE}

\maketitle
\section*{Abstract}
This project investigates the application of machine learning in advertising technology, a rapidly growing field. In order to utilize machine learning in advertising technology however, a balance must be found between accuracy and efficiency. As such, this project will provide a preliminary comparison of several algorithms on the basis of how long they take to fit a large amount of data and how accurate their resulting model is. The algorithms will be used to tackle a binary classification problem, specifically: determining hispanic users from the audience in order to target them for advertising campaigns? =====RESULTS===== In our evaluation of Naive Bayes, Generalized Boosting, Logistic Regression, and Random forest models, we found ...

\begin{multicols}{2}

\section*{Introduction}
Advertising Technology
Background on Advertising Technology use cases.
Background on Cafemom and the specific problem: classify hispanic users

\subsection*{Classifying Rare Events}
(1, pg141) Rare events are more statistically informative that zeros is seen in the variance matrix
(1, p142) When sampling, we must be careful not to select on X differently for the two samples.
(2) The problem is that maximum likelihood estimation of the logistic method is well-known to suffer from small-sample bias. The penalized likelihood or Firth method are the general approach to reducing small-sample bias.

\subsection*{Classification Algorithms}
\subsubsection*{Logistic Regression}
\subsubsection*{Generalized Linear Model}
(4) glmnet fits a generalized linear model via penalized maximum likelihood.
(6) it can deal with all shapes of data, including very large sparse data matrices.
\subsubsection*{Generalized Boosted Model}
(7) Advantages over logistic regression: robust to outliers, can make predictions on missing data, handles unequal class sizes and unbalanced predictor variables, tend to have greater predictive ability
(7) Drawbacks: trees can overfit especially if the number of ending nodes is small or number of trees is too large, definitely want to use cross validation
(7) Use prediction rate as a measure of goodness of fit

\section*{Methods}
\subsection*{Data Collection}
Demdex has Traits\/Segments, uuid and traits\/segments collected
Table generating en\/es language to determine hispanic\/non-hispanic, 1 for es, 0 for en
\begin{itemize}
  \item segment\_hispanic: 80324851 total uuids
  \item segment\_hispanic: 1698878 hispanic (2 percent)
  \item segment\_hispanic2: 9733751 total uuids
  \item segment\_hispanic2: 1868091 hispanic (19 percent)
\end{itemize}
\subsection*{Data Processing}
alldata: 7193606 x 2105480
dataset: 1438721 x 44336
datatrain: 356083 x 18582
datatest: 3597 x 18582
pdata: 0.02, ptest: 0.01

datatrain: 64743 x 6223

datatrain0: 35000 x 10128
datatrain1: 70000 x 10128
datatrain: 142433 x 10128

testdata: 1439 x 10128
Sizes: small100, small250, small500, small1000
Accuracies: small500, small2, small3, small4, small5

Fit - 3 data points: datatrain0, datatrain, datatrain1
  Graph: time vs data, no error bars
Predict -
  Graph: time vs data: smalltest, small500, small1000 (datatrain)
  Graph: accuracy vs train data [errorbars: small500, small2, small3, small4, small5]
Tuning: if you have time

\subsection*{Logistic Regression}
We used \texttt{glmnet} (Generalized Linear Models) to generate logistic regression model.
Then \texttt{predict.glm} is used to predict on the validation sample.
(3) We then determine the how low of a predicted probability is needed to accurately classify the hispanic division.

--Graph: time to model vs. amount of data
--Graph: time to predict vs. number of rows to predict
--Graph: amount of data vs. accuracy
--Table: false positive/negatives

--Tuning Graph

\subsection*{Gradient Boosted Regression Model}
(7) we use distribution = bernoulli since this is a binary classification (use gaussian for adaboost), for n.trees we use a large number of trees that we can prune back later. The shrinkage is the step size, which we chose to be 0.1 since the a smaller step would take longer to model although it would yield better performance. Use cross-validation to determine interaction depth. Decreasing n.minobsinnode increases in-sample fit but risks overfitting. nTrain is used so that you can select the number of trees at the end.
(8) General gbm parameter info

\subsection*{Support Vector Machines}


\subsection*{Naive Bayes}


\subsection*{Random Forest or Ferns}

\subsection*{Comparison}
--Graph: times to model + times to predict (overlayed)
--Graph: accuracy

<<>>=
data(example)
@
The above is a snippet of code used. 

\section*{Results}
Compare time for execution \& accuracy between models\/methods. ex.Unweighted\/Non-filtered\/Logistic Regression



The below is a sample graph of data.
\begin{center}
Something like a plot centered.
\end{center}

\section*{Conclusion}
We have demonstrated the application of machine learning in advertising technology, in particular for a problem with rare events. 

Our results should not be interpreted as a standard comparison of these algorithms. Since most algorithms, and their implementations in R, allow for the tuning of parameters (such as making learning steps smaller and testing various lambda values), without a doubt, the performance of these algorithms could be improved and therefore yield different results from our own research. This project is better served as an example procedure of how machine learning algorithms can be compared.

While the results are encouraging, further exploration of machine learning tools is needed especially for processing large amounts of data with a similarly immense feature space. For example, investigation into SparkR would allow for integration of R's diverse offering of machine learning tools and the parallel processing power of Spark (8). However, these technologies are still very new and undergoing significant amounts of development. Likewise, they will also be limited in their available documentation and resources. Nonetheless, moving forward we expect to see much advancement in machine learning tools to solve problems in advertising technology.

\section*{Acknowledgements}
I would like to thank Patrick McCann at Cafemom for providing guidance as well as allowing the use of Cafemom's advertising data to perform this research. In addition, Professor Mohri has imparted much knowledge on machine learning algorithms in his class, Foundations of Machine Learning (CSCI-GA. 2566-001).

\end{multicols}


\section*{References}
We used \texttt{RStudio Sweave} to build this \LaTeX{} document
\begin{itemize}
  \item 1 http://gking.harvard.edu/files/gking/files/0s.pdf (rare events)
  \item 2 http://www.statisticalhorizons.com/logistic-regression-for-rare-events (rare events)
  \item 4 http://cran.r-project.org/web/packages/glmnet/glmnet.pdf
  \item 3 http://stats.stackexchange.com/questions/25389/obtaining-predicted-values-y-1-or-0-from-a-logistic-regression-model-fit
  \item 5 http://machinelearningmastery.com/an-introduction-to-feature-selection/ (feature selection tips)
  \item best algorithms: http://www.researchgate.net/post/What\_is\_the\_best\_algorithm\_for\_classification\_task
  \item http://en.wikibooks.org/wiki/Data\_Mining\_Algorithms\_In\_R/Classification/SVM (SVM)
  \item http://www.jstatsoft.org/v61/i10/paper (random ferns)
  \item http://www.statmethods.net/advstats/cart.html (rpart and random forest)
  \item 6 http://www.inside-r.org/packages/cran/glmnet/docs/glmnet
  \item 7 http://vimeo.com/71992876 (gbm)
  \item 8 http://datasaucer.blogspot.com/ (R for big data)
  \item 9 http://www.inside-r.org/packages/cran/gbm/docs/gbm
  \item 10 http://jwijffels.github.io/RMOA/doc/pdf/Manual.pdf (MOA manual)
  \item 11 https://github.com/jwijffels/RMOA (streaming RMOA)
\end{itemize}



\end{document}